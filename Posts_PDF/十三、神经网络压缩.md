## 神经网络压缩

### Network pruning

剪枝，直接把没有用的参数直接扔掉。

（1）首先训练一个大模型

（2）评估每一个~~参数的重要性~~ / 神经元的重要性

（3）移除（但是通常来说会掉正确率）

（4）fine-turn（微调模型，回调正确率）然后回到步骤（2）

少量多次，慢慢减小

* 以参数为单位进行裁剪

<img title="" src="file:///C:/Users/lenovo/AppData/Roaming/marktext/images/2025-06-13-14-26-31-image.png" alt="" width="389" data-align="center">

变成了形状不规则的network，实操不好实现。一是模型定义数量都是固定的，二是GPU不好计算不规则的张量。

但经过实际实验，其实加速不明显，不规则计算GPU计算甚至变慢。

* 以神经元为单位进行裁剪

<img title="" src="file:///C:/Users/lenovo/AppData/Roaming/marktext/images/2025-06-13-14-30-41-image.png" alt="" width="423" data-align="center">

实操更简单。一般都是以神经元为单位

* 为什么不直接train一个较小的network？

原因是：大的network是更容易被训练（更容易被optimize），小的network得不到和大网络的精度。

### knowledge distillation

（1）先训练里一个老师网络（大网络），再设置一个学生网络（小网络，老师网络结构的一部分）学习

（2）让学生网络学习老师网络输出的分布

<img title="" src="file:///C:/Users/lenovo/AppData/Roaming/marktext/images/2025-06-13-14-50-36-image.png" alt="" width="363" data-align="center">

当然，老师网络不一定只会是一整个大模型，还可以是Ensemble的值（多个模型输出的平均值）。

* Temperature for softmax

训练技巧是平滑输出分布，统一除以一个未知参数$T$（待学习）。

<img title="" src="file:///C:/Users/lenovo/AppData/Roaming/marktext/images/2025-06-13-15-38-30-image.png" alt="" width="376" data-align="center">

可以让输出分布变得更加平滑，这样可以使学生网络更好学习

### parameter quantization

用较少的空间储存一个参数。

* 用更少的比特替代一个值

* weight clustering

数值相近的分一个群，每一个群都只利用一个数替代。

<img title="" src="file:///C:/Users/lenovo/AppData/Roaming/marktext/images/2025-06-13-14-59-16-image.png" alt="" data-align="center" width="535">

* 用较少的比特表示频繁的簇，用更多的比特表示稀少的簇

终极目标是 只用一个比特（+1 / -1）代表一个参数

### Architecture Design - Depthwise Separable Convolution

（1）Depthwise Convolution：有几个channel就有几个filer，每一个filter就只负责一个channel。输出和输入的channel是一样的。（这样就导致跨channel的特征是看不出来的，因此需要步骤2）

（2）Pointwise Convolution：这里定义限制大小都是$1 \times 1$的filter。channel内部的关系是Depthwise Convolution的工作，Pointwise Convolution只专注于考虑channel的关系。

<img title="" src="file:///C:/Users/lenovo/AppData/Roaming/marktext/images/2025-06-13-15-09-47-image.png" alt="" width="397" data-align="center">

这样子可以缩小参数量

* 原理是Low rank approximation，如下图所示，从$M\times N$变成$N\times K + K \times M$

<img title="" src="file:///C:/Users/lenovo/AppData/Roaming/marktext/images/2025-06-13-15-23-28-image.png" alt="" data-align="center" width="446">

当然这样的作法肯定会增加$W$的限制

因此，Depthwise Convolution具体设计为：

<img title="" src="file:///C:/Users/lenovo/AppData/Roaming/marktext/images/2025-06-13-15-25-56-image.png" alt="" width="408" data-align="center">

### Dynamic Computation

如果准备多个网络结构，就需要大量的储存空间。因此，期望模型可以自己调整计算量（不同设备的运算资源不一样 / 同一个设备不同的电量的运算资源也不一样）

* 网络自己调整网络的深度。通过每一层多加一个layer，判断是否在该层输出。

<img title="" src="file:///C:/Users/lenovo/AppData/Roaming/marktext/images/2025-06-13-15-29-56-image.png" alt="" width="447" data-align="center">

* 网络自己调整自己的网络宽度

![](C:\Users\lenovo\AppData\Roaming\marktext\images\2025-06-13-15-32-10-image.png)

其他运用任务：对于不同的输入，输入的复杂度也需要不同的算力。

<img title="" src="file:///C:/Users/lenovo/AppData/Roaming/marktext/images/2025-06-13-15-33-51-image.png" alt="" width="424" data-align="center">
